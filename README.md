#  Инструкция по работе с сервисом data-api-vtb-backend

Цель закрепить порядок работы с сервисом.

## Пошаговое руководство

1. Загружаем либы питона:
```bash
   pip install -r py/requirements.txt
   pip install -r py/requirements_ml.txt
```
2. Запускаем сервер на питоне:
```bash
   python py/server.py
```
3. Запускаем сервер на go:
```bash
   go run cmd/main.go
```
4. Наблюдаем и пользуемся api по адресу:
   http://localhost:8080/sw/
5. Для запуска телеграм бота перейдите по ссылке:
   https://t.me/MoreTECH_bot


6. Для запуска бота на локальной машине
   запустите файл telegramBot/bot.py

P.S.
Бот развернут на локальной машине участника
и в случае проблем с ботом (не обрабатываются запросы) лучше развернуть у себя

## Алгоритмы и работа над данными.
```
1 Парсинг сырых данных - заголовок новости, текст новости, дата, источник и тэги(далее не использовались)

2 Обработка текста - токенизация, лемматизация и удаление стоп слов
Из особенностей выделим хак с частицой 'не', чтобы не потерять в семантике при обработке, склеивали частицу со следующим словом(например 'не найти' -> 'не_найти')

3 Далее все новости были кластеризованы по заголовкам
Для этого для всех текстов заголовков получили их эмбединги с помощью предобученной модели LaBSE-en-ru
и кластеризовали их  с помощью алгортима HDBSCAN(преимущества - быстрый, сам определяет число кластеров)

Полученные кластеры использовались как основа для формирования дайджеста и трендов с инсайтами

4 Дайджест

Кластеризация по сути дала нам объединение новостей в группе по семантическому смыслу, поэтому для
для формирования дайджеста мы выбрали топ 2(число задается параметрически) кластера по количеству попавших в них новостей.
Из каждого такого кластера брали самый последний по дате заголовок, то есть самый актуальный.

Затем, чтобы добавить в дайджест информацию из самой новости, а не только заголовка был написан алгоритм для поиска
в тексте предложения, отражащающего смысл всей новости. Для этого каждое предложение измеряли с остальными этой новости с помощью косинусной меры, таким образом считали медианное расстояние от каждого до остальных. Предложение с минимальным расстоянием добавляли в дайджест вторым к заголовку как усреднение смысла всей новости(сначала хотели прикрутить генерацию выжимки, но не хватило времени и ресурсов)

5 Тренды и инсайты в них

Опять берем самые большие кластеры как кандидаты для трендов. Для проверки на тренд использовали временной критерий. Для этого сортируем по датам новости и считаем средний временной интервал между ними. Если у кластера новости получили высокую переодичность, то определяем его как трендовый кластер.(В дальнешем планируется добавить статистические критерии на проверку равномерности на основе распределения дат, но пока не успели)
Для тренда целая новость нам не подходит, поэтому для получения тренда в виде фразы или слова использовали алгоритм LDA в качестве key-word extractor(слова векторизовали с помощью tf-idf, с построением n-gram(1,4)) на всех текстах новостей подходящего по временному критерию кластера. Топ результат LDA выводили как тренд.

Для поиска инсайтов использовали найденные ранее и помеченные как трендовые кластеры. Так как инсайт это неявная, скрытая информация в кластере с трендом, то был написан алгоритм для поиска максимально непохожего предложения среди новостей в этом кластере на другие предложение в этом же кластере. Здесь использовался такой же подход со медианным косинусным расстоянием каждого предложения в кластере до других как и в шаге с дайджестом, но бралось самое удаленное по этому расстоянию предложение и выводилось как инсайты.

```


  